{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanHossain/NLPFinalProject/blob/master/Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjc9r0-zQI5",
        "outputId": "0e593be2-43ef-45d3-9fde-2a7bf63778c2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive # import drive from google colab\n",
        "# ROOT = \"/content/drive\"     # default location for the drive\n",
        "# print(ROOT)                 # print content of ROOT (Optional\n",
        "# drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import files\n",
        "import sys\n",
        "from csv import reader\n",
        "from typing import List\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
        "import numpy as np\n",
        "import inflect\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import scipy.io as sp\n",
        "import sklearn.metrics\n",
        "# import numpy as np\n",
        "# import sys\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Initial Setup - Defining the driving variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determines which dataset use and how much to use :\n",
        "# HateSpeech: Column-0 : Sentence, Column-1 : Label [noHate-0, Hate-1]\n",
        "# either 'HateSpeech' or 'KaggleTwitter' or 'TDavidson'\n",
        "dataset_to_use = \"HateSpeech\"\n",
        "dataset_percentage = 100  # percentage range 1 to 100\n",
        "\n",
        "# Initializes file path, column of csv file to parse and\n",
        "# the delimiter for parsing\n",
        "training_file = \"\"\n",
        "test_file = \"\"\n",
        "sentence_column_to_parse = None\n",
        "label_column_to_parse = None\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "delimiter = \",\"\n",
        "if dataset_to_use == \"HateSpeech\":\n",
        "    training_file = \"datasets/hate-speech/train.txt\"\n",
        "    test_file = \"datasets/hate-speech/test.txt\"\n",
        "    delimiter = \"\\t\"\n",
        "    sentence_column_to_parse = 0\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"KaggleTwitter\":\n",
        "    training_file = \"datasets/kaggle-twitter/train.csv\"\n",
        "    test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 2\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"TDavidson\":\n",
        "    training_file = \"datasets/t-davidson-hate-speech/labeled_data.csv\"\n",
        "    # TODO: Update test path for this dataset\n",
        "    # test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 6\n",
        "    label_column_to_parse = 2\n",
        "else:\n",
        "    print(\"Invalid Dataset specified\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seven tasks are done as part of this:\n",
        "  - lower word case\n",
        "  - remove stopwords\n",
        "  - remove punctuation\n",
        "  - convert numbers to texts\n",
        "  - perform stemming\n",
        "  - Add - \\<s> and \\</s> for every sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_numbers(sentence:List[str]) -> List[str]:\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words\n",
        "    with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []  \n",
        "    for word in sentence.split():\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def stem_words(sentence: List[str]) -> List[str]:\n",
        "    \"\"\"Stems the given sentence\n",
        "\n",
        "    Args:\n",
        "        sentence (list): words to be stemmed\n",
        "\n",
        "    Returns:\n",
        "        str: stemmed sentence\n",
        "    \"\"\"\n",
        "    stemmed_words = []\n",
        "    for word in sentence.split():\n",
        "        stemmed_words.append(lancaster.stem(word))\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def preprocessing(running_lines: List[str]) -> List[str]:\n",
        "    \"\"\"This function takes in the running test and return back the\n",
        "    preprocessed text. Six tasks are done as part of this:\n",
        "      1. lower word case\n",
        "      2. remove stopwords\n",
        "      3. remove punctuation\n",
        "      4. convert numbers to texts\n",
        "      5. perform stemming\n",
        "\n",
        "    Args:\n",
        "        sentence (List[str]): list of lines\n",
        "\n",
        "    Returns:\n",
        "        List[str]: list of sentences which are processed\n",
        "    \"\"\"\n",
        "    preprocessed_lines = []\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    for line in running_lines:\n",
        "        # lower case\n",
        "        lower_case_data = line.lower()\n",
        "        # remove stop words\n",
        "        data_without_stop_word = remove_stopwords(lower_case_data)\n",
        "        # remove punctunation\n",
        "        data_without_punct = strip_punctuation(data_without_stop_word)\n",
        "        # replace numbers '1' to 'one'\n",
        "        processed_data = replace_numbers(data_without_punct)\n",
        "        # stem words\n",
        "        processed_data = stem_words(processed_data)\n",
        "        # add start and stop tags\n",
        "        # processed_data.insert(0, \"<s>\")\n",
        "        # processed_data.append(\"</s>\")\n",
        "        preprocessed_lines.append(processed_data)\n",
        "    return preprocessed_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_data(training_file_path: str, percentage: int,\n",
        "               sentence_column: int, label_column: int,\n",
        "               delimit: str):\n",
        "    \"\"\"This function is used to parse input lines\n",
        "    and returns a the provided percent of data.\n",
        "\n",
        "    Args:\n",
        "        lines (List[str]): list of lines\n",
        "        percentage (int): percent of the dataset needed\n",
        "        sentence_column (int): sentence column from the dataset\n",
        "        label_column (int): label column from the dataset\n",
        "        delimit (str): delimiter\n",
        "    Returns:\n",
        "        List[str], List[str]: examples , labels -> [percentage of dataset]\n",
        "    \"\"\"\n",
        "    percentage_sentences = []\n",
        "    percentage_labels = []\n",
        "    with open(training_file_path, \"r\", encoding=\"utf8\",\n",
        "              errors=\"ignore\") as csvfile:\n",
        "        read_sentences = []\n",
        "        label_sentences = []\n",
        "        csv_reader = reader(csvfile, delimiter=delimit)\n",
        "        # skipping header\n",
        "        header = next(csv_reader)\n",
        "        # line_length = len(list(csv_reader_copy))\n",
        "        if header is not None:\n",
        "            for row in csv_reader:\n",
        "                read_sentences.append(row[sentence_column])\n",
        "                label_sentences.append(row[label_column])\n",
        "        end_of_data = int(len(read_sentences) * percentage * .01)\n",
        "        percentage_sentences = read_sentences[0:end_of_data]\n",
        "        percentage_labels = label_sentences[0:end_of_data]\n",
        "    return percentage_sentences, percentage_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Improved - BiLSTM on HateSpeech with 100 % data \n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_labels = parse_data(training_file,\n",
        "                                           dataset_percentage,\n",
        "                                           sentence_column_to_parse,\n",
        "                                           label_column_to_parse,\n",
        "                                           delimiter)\n",
        "# parse and preprocess the data\n",
        "processed_train_sentences = preprocessing(train_sentences)\n",
        "# verify the processed sentences\n",
        "# for sentence in sentences:\n",
        "#     print(sentence)\n",
        "# This is the baseline classifier\n",
        "print(\n",
        "    f\"Performing Improved - BiLSTM on {dataset_to_use}\"\n",
        "    f\" with {dataset_percentage} % data \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Generating word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this step, we intend to use the Keras library to build a recurrent neural network based on bidirectional LSTMs. The model will take word embeddings as input so we will use pre-trained GloVe embeddings to make the embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sentence_word_embeddings(X_train_sentences:List[str]):\n",
        "    \"\"\"Converts the sentences into word embeddings.\n",
        "\n",
        "    Args:\n",
        "        X_train_sentences (List[str]): list of training sentences\n",
        "\n",
        "    Returns:\n",
        "        tuple: word embeddings for each sentence, vocab size and embedding dictionary\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    text = np.array(X_train_sentences)\n",
        "    tokenizer.fit_on_texts(X_train_sentences)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    embeddings_dict = {}\n",
        "    file_embeddings = open(\"utils/glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
        "    for embedding_line in file_embeddings:\n",
        "        values = embedding_line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_dict[word] = coefs\n",
        "    file_embeddings.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (text, word_index, embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(processed_train_sentences)\n",
        "\n",
        "#First one is sentence embeddings\n",
        "#Second one is size of vocab\n",
        "#Third is embeddings dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Word index of the word testing is :  3039\n",
            "\n",
            " Embedding for the word want \n",
            " \n",
            " [ 3.7413e-01  7.1623e-01 -3.5810e-01  7.7834e-02 -9.2033e-01 -3.8558e-01\n",
            "  5.3116e-01 -5.1657e-01  1.2817e-01  2.6236e-01 -2.7895e-01  2.5293e-01\n",
            " -5.5937e+00 -5.7123e-01 -9.1184e-01  1.3920e-01 -5.0184e-01 -1.3487e-01\n",
            " -6.3133e-01  1.4302e-01  6.3659e-01  2.1426e-01  3.9087e-01  8.3384e-01\n",
            "  7.4350e-01  3.7236e-01  5.9994e-01  2.7211e-01  5.7034e-02 -1.3236e+00\n",
            "  1.5121e-01  4.6511e-01 -1.3622e-03  3.6893e-02  7.5773e-01 -5.9615e-01\n",
            "  8.3311e-02 -3.2657e-01 -1.1019e-02  2.9374e-01 -1.1935e+00 -3.9529e-01\n",
            "  1.8498e-01 -6.4563e-01  6.0677e-01 -7.2177e-01  7.4921e-01  9.1771e-02\n",
            " -9.6784e-02  3.3105e-01]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'builtin_function_or_method' object is not iterable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/gx/v870d0zd6jg565qly4vplfx40000gn/T/ipykernel_21408/1866849043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mword_index_to_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membeddings_dict_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mword_index_to_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0membeddings_dict_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
          ]
        }
      ],
      "source": [
        "## Check function\n",
        "x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "# X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(x_train_sample)\n",
        "# print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "print(\"\\n Word index of the word testing is : \", word_index_s[\"industry\"])\n",
        "print(\"\\n Embedding for the word want \\n \\n\", embeddings_dict_s[\"want\"])\n",
        "len(embeddings_dict_s[\"want\"])\n",
        "\n",
        "word_index_to_embedding = {}\n",
        "\n",
        "for i in embeddings_dict_s.keys():\n",
        "    print(i)\n",
        "    word_index_to_embedding[word_index_s[i]] =  embeddings_dict_s[i]\n",
        "\n",
        "print(word_index_to_embedding[0])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/gx/v870d0zd6jg565qly4vplfx40000gn/T/ipykernel_21408/3448526991.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_Glove_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "def converge_to_binary(x):\n",
        "    x = np.where(x < .5, -1, 1)\n",
        "    return x\n",
        "\n",
        "def converge_to_prob(x):\n",
        "    x = np.where(x < 0., 0., 1.)\n",
        "    return x\n",
        "\n",
        "# Defining input size, hidden layer size, output size and batch size respectively\n",
        "n_in, n_h, n_out, hidden_layers, activation_function = 50, 10, 1, 1, 'lstm'\n",
        "\n",
        "x = torch.tensor(X_train_Glove_s).float()\n",
        "y = torch.tensor(train_labels).float()\n",
        "\n",
        "class MyModule(nn.Module):\n",
        "    def __init__(self, \n",
        "            n_in, \n",
        "            neurons_per_hidden, \n",
        "            n_out,\n",
        "            hidden_layers, \n",
        "            activation_function,\n",
        "            embeddings\n",
        "        ):\n",
        "\n",
        "        super(MyModule, self).__init__()\n",
        "\n",
        "        self.n_in = n_in\n",
        "        self.n_h = neurons_per_hidden\n",
        "        self.n_out = n_out\n",
        "        self.h_l = hidden_layers\n",
        "\n",
        "        self.a_f = activation_function\n",
        "\n",
        "        self.embeddings = embeddings\n",
        "\n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.input = nn.Linear(n_in, self.n_h)\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding_layer = nn.Embedding(num_embedding=len(self.embeddings), embedding_dim=n_in)\n",
        "        # Load weights into embedding layer\n",
        "        self.embedding_layer.load_state_dict({'weight': self.embeddings})\n",
        "        # Do not train embedding layer\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "        #probably need a variable for recurrent dropout and dropout layer dropout\n",
        "\n",
        "        # Defaults to Relu if activation_function is improperly sp\n",
        "        self.activation_layer = nn.ReLU()\n",
        "\n",
        "        if activation_function == 'relu':\n",
        "            self.activation_layer = nn.ReLU()\n",
        "        elif activation_function == 'tanh':\n",
        "            self.activation_layer = nn.Tanh()\n",
        "        elif activation_function == 'sigmoid':\n",
        "            self.activation_layer == nn.Sigmoid()\n",
        "        elif activation_function == 'identity':\n",
        "            self.activation_layer == nn.Identity()\n",
        "        elif activation_function == 'lstm':\n",
        "            self.activation_layer == nn.LSTM(bidirectional=True, dropout=0.2)\n",
        "        else:\n",
        "            print(\"Invalid activation function specified\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # self.linears = nn.ModuleList([nn.Linear(self.n_h, self.n_h) for i in range(self.h_l - 1)])\n",
        "        self.activation_layers = nn.ModuleList([self.activation_layer for i in range(self.h_l - 1)])\n",
        "        self.dropout_layers = nn.ModuleList([nn.Dropout(0.5) for i in range(self.h_l - 1)])\n",
        "        \n",
        "        # Output layer, 10 units - one for each digit\n",
        "        # self.output = nn.Linear(self.n_h, n_out)\n",
        "        self.output == nn.ReLU()\n",
        "        # Define sigmoid output\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.input(x)\n",
        "        x = self.activation_layer(x)\n",
        "\n",
        "        # ModuleList can act as an iterable, or be indexed using ints\n",
        "        for i, l in enumerate(self.linears):\n",
        "            # x = self.linears[i // 2](x) + l(x)\n",
        "            x = self.activation_layers[i // 2](x) + l(x)\n",
        "            x = self.dropout_layers[i // 2](x) + l(x)\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = nn.Sequential(MyModule(n_in, n_h, n_out, hidden_layers, activation_function))\n",
        "print(model)\n",
        "\n",
        "# Construct the loss function\n",
        "criterion = torch.nn.BCELoss()\n",
        "# Construct the optimizer (Adam in this case)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.05)\n",
        "\n",
        "# Optimization\n",
        "for epoch in range(50):\n",
        "   # Forward pass: Compute predicted y by passing x to the model\n",
        "   y_pred = model(x)\n",
        "\n",
        "   y_pred_numpy = y_pred.detach().numpy()\n",
        "   y_pred_tanh_range = converge_to_binary(y_pred_numpy)\n",
        "   y_numpy = y.detach().numpy()\n",
        "#    y_sigmoid_range = converge_to_prob(y_numpy)\n",
        "\n",
        "#    print(sklearn.metrics.accuracy_score(y_pred_numpy, y))\n",
        "\n",
        "   # Compute and print loss\n",
        "   loss = criterion(y_pred, torch.tensor(y_numpy).float())\n",
        "#    print('epoch: ', epoch,' loss: ', loss.item())\n",
        "\n",
        "   # Zero gradients, perform a backward pass, and update the weights.\n",
        "   optimizer.zero_grad()\n",
        "\n",
        "   # perform a backward pass (backpropagation)\n",
        "   loss.backward()\n",
        "\n",
        "   # Update the parameters\n",
        "   optimizer.step()\n",
        "\n",
        "# Plotting training data\n",
        "# plot_data(X, Y)\n",
        "\n",
        "y_pred_tanh_range_from_train = y_pred_tanh_range\n",
        "\n",
        "y_pred = model(x_test)\n",
        "y_pred_numpy = y_pred.detach().numpy()\n",
        "y_pred_tanh_range = converge_to_binary(y_pred_numpy)\n",
        "\n",
        "# print(\"Training Accuracy\")\n",
        "# print(sklearn.metrics.accuracy_score(y_pred_tanh_range_from_train, y))\n",
        "\n",
        "# print(\"Testing Accuracy\")\n",
        "# print(sklearn.metrics.accuracy_score(y_pred_tanh_range, y_test))\n",
        "\n",
        "# Plotting the test data\n",
        "# plot_data(x_test, y_pred_tanh_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Script.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
