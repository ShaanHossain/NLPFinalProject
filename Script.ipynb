{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanHossain/NLPFinalProject/blob/master/Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjc9r0-zQI5",
        "outputId": "0e593be2-43ef-45d3-9fde-2a7bf63778c2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive # import drive from google colab\n",
        "# ROOT = \"/content/drive\"     # default location for the drive\n",
        "# print(ROOT)                 # print content of ROOT (Optional\n",
        "# drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import files\n",
        "import sys\n",
        "from csv import reader\n",
        "from typing import List\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
        "import numpy as np\n",
        "import inflect\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Initial Setup - Defining the driving variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determines which dataset use and how much to use :\n",
        "# HateSpeech: Column-0 : Sentence, Column-1 : Label [noHate-0, Hate-1]\n",
        "# either 'HateSpeech' or 'KaggleTwitter' or 'TDavidson'\n",
        "dataset_to_use = \"HateSpeech\"\n",
        "dataset_percentage = 100  # percentage range 1 to 100\n",
        "\n",
        "# Initializes file path, column of csv file to parse and\n",
        "# the delimiter for parsing\n",
        "training_file = \"\"\n",
        "test_file = \"\"\n",
        "sentence_column_to_parse = None\n",
        "label_column_to_parse = None\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "delimiter = \",\"\n",
        "if dataset_to_use == \"HateSpeech\":\n",
        "    training_file = \"datasets/hate-speech/train.txt\"\n",
        "    test_file = \"datasets/hate-speech/test.txt\"\n",
        "    delimiter = \"\\t\"\n",
        "    sentence_column_to_parse = 0\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"KaggleTwitter\":\n",
        "    training_file = \"datasets/kaggle-twitter/train.csv\"\n",
        "    test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 2\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"TDavidson\":\n",
        "    training_file = \"datasets/t-davidson-hate-speech/labeled_data.csv\"\n",
        "    # TODO: Update test path for this dataset\n",
        "    # test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 6\n",
        "    label_column_to_parse = 2\n",
        "else:\n",
        "    print(\"Invalid Dataset specified\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seven tasks are done as part of this:\n",
        "  - lower word case\n",
        "  - remove stopwords\n",
        "  - remove punctuation\n",
        "  - convert numbers to texts\n",
        "  - perform stemming\n",
        "  - Add - \\<s> and \\</s> for every sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_numbers(sentence:List[str]) -> List[str]:\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words\n",
        "    with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []  \n",
        "    for word in sentence.split():\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def stem_words(sentence: List[str]) -> List[str]:\n",
        "    \"\"\"Stems the given sentence\n",
        "\n",
        "    Args:\n",
        "        sentence (list): words to be stemmed\n",
        "\n",
        "    Returns:\n",
        "        str: stemmed sentence\n",
        "    \"\"\"\n",
        "    stemmed_words = []\n",
        "    for word in sentence.split():\n",
        "        stemmed_words.append(lancaster.stem(word))\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def preprocessing(running_lines: List[str]) -> List[str]:\n",
        "    \"\"\"This function takes in the running test and return back the\n",
        "    preprocessed text. Six tasks are done as part of this:\n",
        "      1. lower word case\n",
        "      2. remove stopwords\n",
        "      3. remove punctuation\n",
        "      4. convert numbers to texts\n",
        "      5. perform stemming\n",
        "\n",
        "    Args:\n",
        "        sentence (List[str]): list of lines\n",
        "\n",
        "    Returns:\n",
        "        List[str]: list of sentences which are processed\n",
        "    \"\"\"\n",
        "    preprocessed_lines = []\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    for line in running_lines:\n",
        "        # lower case\n",
        "        lower_case_data = line.lower()\n",
        "        # remove stop words\n",
        "        data_without_stop_word = remove_stopwords(lower_case_data)\n",
        "        # remove punctunation\n",
        "        data_without_punct = strip_punctuation(data_without_stop_word)\n",
        "        # replace numbers '1' to 'one'\n",
        "        processed_data = replace_numbers(data_without_punct)\n",
        "        # stem words\n",
        "        processed_data = stem_words(processed_data)\n",
        "        # add start and stop tags\n",
        "        # processed_data.insert(0, \"<s>\")\n",
        "        # processed_data.append(\"</s>\")\n",
        "        preprocessed_lines.append(processed_data)\n",
        "    return preprocessed_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_data(training_file_path: str, percentage: int,\n",
        "               sentence_column: int, label_column: int,\n",
        "               delimit: str):\n",
        "    \"\"\"This function is used to parse input lines\n",
        "    and returns a the provided percent of data.\n",
        "\n",
        "    Args:\n",
        "        lines (List[str]): list of lines\n",
        "        percentage (int): percent of the dataset needed\n",
        "        sentence_column (int): sentence column from the dataset\n",
        "        label_column (int): label column from the dataset\n",
        "        delimit (str): delimiter\n",
        "    Returns:\n",
        "        List[str], List[str]: examples , labels -> [percentage of dataset]\n",
        "    \"\"\"\n",
        "    percentage_sentences = []\n",
        "    percentage_labels = []\n",
        "    with open(training_file_path, \"r\", encoding=\"utf8\",\n",
        "              errors=\"ignore\") as csvfile:\n",
        "        read_sentences = []\n",
        "        label_sentences = []\n",
        "        csv_reader = reader(csvfile, delimiter=delimit)\n",
        "        # skipping header\n",
        "        header = next(csv_reader)\n",
        "        # line_length = len(list(csv_reader_copy))\n",
        "        if header is not None:\n",
        "            for row in csv_reader:\n",
        "                read_sentences.append(row[sentence_column])\n",
        "                label_sentences.append(row[label_column])\n",
        "        end_of_data = int(len(read_sentences) * percentage * .01)\n",
        "        percentage_sentences = read_sentences[0:end_of_data]\n",
        "        percentage_labels = label_sentences[0:end_of_data]\n",
        "    return percentage_sentences, percentage_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Improved - BiLSTM on HateSpeech with 100 % data \n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_labels = parse_data(training_file,\n",
        "                                           dataset_percentage,\n",
        "                                           sentence_column_to_parse,\n",
        "                                           label_column_to_parse,\n",
        "                                           delimiter)\n",
        "# parse and preprocess the data\n",
        "processed_train_sentences = preprocessing(train_sentences)\n",
        "# verify the processed sentences\n",
        "# for sentence in sentences:\n",
        "#     print(sentence)\n",
        "# This is the baseline classifier\n",
        "print(\n",
        "    f\"Performing Improved - BiLSTM on {dataset_to_use}\"\n",
        "    f\" with {dataset_percentage} % data \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Generating word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this step, we intend to use the Keras library to build a recurrent neural network based on bidirectional LSTMs. The model will take word embeddings as input so we will use pre-trained GloVe embeddings to make the embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sentence_word_embeddings(X_train_sentences:List[str]):\n",
        "    \"\"\"Converts the sentences into word embeddings.\n",
        "\n",
        "    Args:\n",
        "        X_train_sentences (List[str]): list of training sentences\n",
        "\n",
        "    Returns:\n",
        "        tuple: word embeddings for each sentence, vocab size and embedding dictionary\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    text = np.array(X_train_sentences)\n",
        "    tokenizer.fit_on_texts(X_train_sentences)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    embeddings_dict = {}\n",
        "    file_embeddings = open(\"utils/glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
        "    for embedding_line in file_embeddings:\n",
        "        values = embedding_line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_dict[word] = coefs\n",
        "    file_embeddings.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (X_train_sentences, word_index, embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3951 unique tokens.\n",
            "Total 1193514 word vectors.\n"
          ]
        }
      ],
      "source": [
        "X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(processed_train_sentences)\n",
        "\n",
        "## Check function\n",
        "# x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "# X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(x_train_sample)\n",
        "# print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "# print(\"\\n Word index of the word testing is : \", word_index_s[\"industry\"])\n",
        "# print(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Script.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
