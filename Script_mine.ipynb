{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanHossain/NLPFinalProject/blob/master/Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjc9r0-zQI5",
        "outputId": "0e593be2-43ef-45d3-9fde-2a7bf63778c2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive # import drive from google colab\n",
        "# ROOT = \"/content/drive\"     # default location for the drive\n",
        "# print(ROOT)                 # print content of ROOT (Optional\n",
        "# drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import files\n",
        "import sys\n",
        "from csv import reader\n",
        "from typing import List\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
        "import numpy as np\n",
        "import inflect\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Initial Setup - Defining the driving variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determines which dataset use and how much to use :\n",
        "# HateSpeech: Column-0 : Sentence, Column-1 : Label [noHate-0, Hate-1]\n",
        "# either 'HateSpeech' or 'KaggleTwitter' or 'TDavidson'\n",
        "dataset_to_use = \"HateSpeech\"\n",
        "dataset_percentage = 100  # percentage range 1 to 100\n",
        "\n",
        "# Initializes file path, column of csv file to parse and\n",
        "# the delimiter for parsing\n",
        "training_file = \"\"\n",
        "test_file = \"\"\n",
        "sentence_column_to_parse = None\n",
        "label_column_to_parse = None\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "delimiter = \",\"\n",
        "if dataset_to_use == \"HateSpeech\":\n",
        "    training_file = \"datasets/hate-speech/train.txt\"\n",
        "    test_file = \"datasets/hate-speech/test.txt\"\n",
        "    delimiter = \"\\t\"\n",
        "    sentence_column_to_parse = 0\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"KaggleTwitter\":\n",
        "    training_file = \"datasets/kaggle-twitter/train.csv\"\n",
        "    test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 2\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"TDavidson\":\n",
        "    training_file = \"datasets/t-davidson-hate-speech/labeled_data.csv\"\n",
        "    # TODO: Update test path for this dataset\n",
        "    # test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 6\n",
        "    label_column_to_parse = 2\n",
        "else:\n",
        "    print(\"Invalid Dataset specified\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seven tasks are done as part of this:\n",
        "  - lower word case\n",
        "  - remove stopwords\n",
        "  - remove punctuation\n",
        "  - convert numbers to texts\n",
        "  - perform stemming\n",
        "  - Add - \\<s> and \\</s> for every sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_numbers(sentence:List[str]) -> List[str]:\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words\n",
        "    with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []  \n",
        "    for word in sentence.split():\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def stem_words(sentence: List[str]) -> List[str]:\n",
        "    \"\"\"Stems the given sentence\n",
        "\n",
        "    Args:\n",
        "        sentence (list): words to be stemmed\n",
        "\n",
        "    Returns:\n",
        "        str: stemmed sentence\n",
        "    \"\"\"\n",
        "    stemmed_words = []\n",
        "    for word in sentence.split():\n",
        "        stemmed_words.append(lancaster.stem(word))\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def preprocessing(running_lines: List[str]) -> List[str]:\n",
        "    \"\"\"This function takes in the running test and return back the\n",
        "    preprocessed text. Six tasks are done as part of this:\n",
        "      1. lower word case\n",
        "      2. remove stopwords\n",
        "      3. remove punctuation\n",
        "      4. convert numbers to texts\n",
        "      5. perform stemming\n",
        "\n",
        "    Args:\n",
        "        sentence (List[str]): list of lines\n",
        "\n",
        "    Returns:\n",
        "        List[str]: list of sentences which are processed\n",
        "    \"\"\"\n",
        "    preprocessed_lines = []\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    for line in running_lines:\n",
        "        # lower case\n",
        "        lower_case_data = line.lower()\n",
        "        # remove stop words\n",
        "        data_without_stop_word = remove_stopwords(lower_case_data)\n",
        "        # remove punctunation\n",
        "        data_without_punct = strip_punctuation(data_without_stop_word)\n",
        "        # replace numbers '1' to 'one'\n",
        "        processed_data = replace_numbers(data_without_punct)\n",
        "        # stem words\n",
        "        processed_data = stem_words(processed_data)\n",
        "        # add start and stop tags\n",
        "        # processed_data.insert(0, \"<s>\")\n",
        "        # processed_data.append(\"</s>\")\n",
        "        preprocessed_lines.append(processed_data)\n",
        "    return preprocessed_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_data(training_file_path: str, percentage: int,\n",
        "               sentence_column: int, label_column: int,\n",
        "               delimit: str):\n",
        "    \"\"\"This function is used to parse input lines\n",
        "    and returns a the provided percent of data.\n",
        "\n",
        "    Args:\n",
        "        lines (List[str]): list of lines\n",
        "        percentage (int): percent of the dataset needed\n",
        "        sentence_column (int): sentence column from the dataset\n",
        "        label_column (int): label column from the dataset\n",
        "        delimit (str): delimiter\n",
        "    Returns:\n",
        "        List[str], List[str]: examples , labels -> [percentage of dataset]\n",
        "    \"\"\"\n",
        "    percentage_sentences = []\n",
        "    percentage_labels = []\n",
        "    with open(training_file_path, \"r\", encoding=\"utf8\",\n",
        "              errors=\"ignore\") as csvfile:\n",
        "        read_sentences = []\n",
        "        label_sentences = []\n",
        "        csv_reader = reader(csvfile, delimiter=delimit)\n",
        "        # skipping header\n",
        "        header = next(csv_reader)\n",
        "        # line_length = len(list(csv_reader_copy))\n",
        "        if header is not None:\n",
        "            for row in csv_reader:\n",
        "                read_sentences.append(row[sentence_column])\n",
        "                label_sentences.append(int(row[label_column]))\n",
        "        end_of_data = int(len(read_sentences) * percentage * .01)\n",
        "        percentage_sentences = read_sentences[0:end_of_data]\n",
        "        percentage_labels = label_sentences[0:end_of_data]\n",
        "    return percentage_sentences, percentage_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Improved - BiLSTM on HateSpeech with 100 % data \n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_labels = parse_data(training_file,\n",
        "                                           dataset_percentage,\n",
        "                                           sentence_column_to_parse,\n",
        "                                           label_column_to_parse,\n",
        "                                           delimiter)\n",
        "# parse and preprocess the data\n",
        "processed_train_sentences = preprocessing(train_sentences)\n",
        "# verify the processed sentences\n",
        "# for sentence in sentences:\n",
        "#     print(sentence)\n",
        "# This is the baseline classifier\n",
        "print(\n",
        "    f\"Performing Improved - BiLSTM on {dataset_to_use}\"\n",
        "    f\" with {dataset_percentage} % data \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Generating word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this step, we intend to use the Keras library to build a recurrent neural network based on bidirectional LSTMs. The model will take word embeddings as input so we will use pre-trained GloVe embeddings to make the embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sentence_word_embeddings(X_train_sentences:List[str]):\n",
        "    \"\"\"Converts the sentences into word embeddings.\n",
        "\n",
        "    Args:\n",
        "        X_train_sentences (List[str]): list of training sentences\n",
        "\n",
        "    Returns:\n",
        "        tuple: word embeddings for each sentence, vocab size and embedding dictionary\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    text = np.array(X_train_sentences)\n",
        "    tokenizer.fit_on_texts(X_train_sentences)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    embeddings_dict = {}\n",
        "    file_embeddings = open(\"utils/glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
        "    for embedding_line in file_embeddings:\n",
        "        values = embedding_line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_dict[word] = coefs\n",
        "    file_embeddings.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (text, word_index, embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3951 unique tokens.\n",
            "Total 1193514 word vectors.\n"
          ]
        }
      ],
      "source": [
        "X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(processed_train_sentences)\n",
        "embedding_size = len(X_train_Glove_s[0])\n",
        "## Check function\n",
        "# x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "# X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(x_train_sample)\n",
        "# print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "# print(\"\\n Word index of the word testing is : \", word_index_s[\"industry\"])\n",
        "# print(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Defining function to create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "import optuna\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bilstm(\n",
        "    word_index, \n",
        "    embeddings_dict,   \n",
        "    embedding_dim,\n",
        "    num_hidden_layers,\n",
        "    num_nodes_per_hidden_layer,\n",
        "    num_nodes_final_fc_layer,\n",
        "    input_dropout,\n",
        "    recurrent_dropout,\n",
        "    output_dropout,\n",
        "    learning_rate,\n",
        "    max_sequence_length, \n",
        "    nclasses=2\n",
        "    ):\n",
        "\n",
        "    model = Sequential()\n",
        "    # Make the embedding matrix using the embedding_dict\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, embedding_dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" embedding_dim is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    # Add embedding layer\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                embedding_dim,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=max_sequence_length,\n",
        "                                trainable=True))\n",
        "    # Add hidden layers \n",
        "    for i in range(0,num_hidden_layers):\n",
        "        # Add a bidirectional lstm layer\n",
        "        model.add(Bidirectional(LSTM(num_nodes_per_hidden_layer, return_sequences=True, recurrent_dropout=recurrent_dropout, dropout=input_dropout)))\n",
        "        # Add a dropout layer after each lstm layer\n",
        "        model.add(Dropout(output_dropout))\n",
        "    model.add(Bidirectional(LSTM(num_nodes_per_hidden_layer, recurrent_dropout=recurrent_dropout, dropout=input_dropout)))\n",
        "    model.add(Dropout(output_dropout))\n",
        "    # Add the fully connected layer with 256 nurons and relu activation\n",
        "    model.add(Dense(num_nodes_final_fc_layer, activation='relu'))\n",
        "    # Add the output layer with softmax activation since we have 2 classes\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    # Compile the model using sparse_categorical_crossentropy\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    # keras.optimizers.ada\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building Model!\n"
          ]
        }
      ],
      "source": [
        "print(\"Building Model!\")\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    model = build_bilstm(\n",
        "        word_index=word_index_s, \n",
        "        embeddings_dict=embeddings_dict_s, \n",
        "        embedding_dim=50, \n",
        "        num_hidden_layers=3, \n",
        "        num_nodes_per_hidden_layer=32,  \n",
        "        num_nodes_final_fc_layer=256,\n",
        "        input_dropout=0,\n",
        "        recurrent_dropout=.2,\n",
        "        output_dropout=.5,\n",
        "        learning_rate=trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
        "        max_sequence_length=embedding_size)\n",
        "    # model.summary()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_train_Glove_s, train_labels, test_size = 0.2)\n",
        "    data_history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                           validation_data=(np.array(X_test),np.array(y_test)),\n",
        "                           epochs=3,\n",
        "                           batch_size=128,\n",
        "                           verbose=1)\n",
        "    \n",
        "    score = model.evaluate(np.array(X_train), np.array(y_train), verbose=0)\n",
        "    return score[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:31:51,144]\u001b[0m A new study created in memory with name: no-name-6dab752c-5249-4d51-a3c1-2ecf033a234d\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 577ms/step - loss: 0.6786 - accuracy: 0.5595 - val_loss: 0.6658 - val_accuracy: 0.6292\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 486ms/step - loss: 0.6438 - accuracy: 0.6490 - val_loss: 0.6437 - val_accuracy: 0.6423\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 483ms/step - loss: 0.5558 - accuracy: 0.7163 - val_loss: 0.5791 - val_accuracy: 0.6971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:32:24,033]\u001b[0m Trial 0 finished with value: 0.7790849804878235 and parameters: {'lr': 0.0025748915312877744}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 573ms/step - loss: 2.2608 - accuracy: 0.4876 - val_loss: 0.6945 - val_accuracy: 0.5065\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 481ms/step - loss: 0.6974 - accuracy: 0.4791 - val_loss: 0.6932 - val_accuracy: 0.4935\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 493ms/step - loss: 0.6938 - accuracy: 0.4863 - val_loss: 0.6932 - val_accuracy: 0.4935\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:32:57,509]\u001b[0m Trial 1 finished with value: 0.501960813999176 and parameters: {'lr': 0.06174678166167813}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 589ms/step - loss: 0.6960 - accuracy: 0.4752 - val_loss: 0.6945 - val_accuracy: 0.5170\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 497ms/step - loss: 0.6951 - accuracy: 0.4902 - val_loss: 0.6935 - val_accuracy: 0.5144\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 502ms/step - loss: 0.6943 - accuracy: 0.4948 - val_loss: 0.6929 - val_accuracy: 0.4961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:33:31,522]\u001b[0m Trial 2 finished with value: 0.4986928105354309 and parameters: {'lr': 3.487188629151513e-05}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 593ms/step - loss: 0.6948 - accuracy: 0.4922 - val_loss: 0.6929 - val_accuracy: 0.4856\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 523ms/step - loss: 0.6933 - accuracy: 0.5039 - val_loss: 0.6869 - val_accuracy: 0.6188\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 497ms/step - loss: 0.6899 - accuracy: 0.5497 - val_loss: 0.6825 - val_accuracy: 0.6292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:34:05,461]\u001b[0m Trial 3 finished with value: 0.584967315196991 and parameters: {'lr': 9.303640497130201e-05}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 18s 620ms/step - loss: 0.6921 - accuracy: 0.5261 - val_loss: 0.6851 - val_accuracy: 0.5849\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 517ms/step - loss: 0.6828 - accuracy: 0.5863 - val_loss: 0.6762 - val_accuracy: 0.5796\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 498ms/step - loss: 0.6714 - accuracy: 0.5993 - val_loss: 0.6648 - val_accuracy: 0.5875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:34:40,139]\u001b[0m Trial 4 finished with value: 0.6215686202049255 and parameters: {'lr': 0.00025801609868448736}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 18s 632ms/step - loss: 0.6785 - accuracy: 0.5706 - val_loss: 0.6595 - val_accuracy: 0.5849\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 7s 576ms/step - loss: 0.6617 - accuracy: 0.6170 - val_loss: 0.6325 - val_accuracy: 0.6136\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 535ms/step - loss: 0.6314 - accuracy: 0.6549 - val_loss: 0.5953 - val_accuracy: 0.6815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:35:16,027]\u001b[0m Trial 5 finished with value: 0.6954248547554016 and parameters: {'lr': 0.0010870374825235974}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 592ms/step - loss: 0.6882 - accuracy: 0.5451 - val_loss: 0.6834 - val_accuracy: 0.5979\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 531ms/step - loss: 0.6823 - accuracy: 0.5817 - val_loss: 0.6738 - val_accuracy: 0.6057\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 7s 543ms/step - loss: 0.6781 - accuracy: 0.5830 - val_loss: 0.6661 - val_accuracy: 0.6005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:35:50,910]\u001b[0m Trial 6 finished with value: 0.613725483417511 and parameters: {'lr': 0.00021697420937311787}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 16s 593ms/step - loss: 0.6876 - accuracy: 0.5359 - val_loss: 0.6756 - val_accuracy: 0.5901\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 495ms/step - loss: 0.6672 - accuracy: 0.5987 - val_loss: 0.6542 - val_accuracy: 0.6188\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 498ms/step - loss: 0.6558 - accuracy: 0.6092 - val_loss: 0.6392 - val_accuracy: 0.6449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:36:23,833]\u001b[0m Trial 7 finished with value: 0.6313725709915161 and parameters: {'lr': 0.0007509833164117265}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 596ms/step - loss: 0.6853 - accuracy: 0.5510 - val_loss: 0.6838 - val_accuracy: 0.5091\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 545ms/step - loss: 0.6716 - accuracy: 0.5752 - val_loss: 0.6426 - val_accuracy: 0.6475\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 500ms/step - loss: 0.6560 - accuracy: 0.6163 - val_loss: 0.6180 - val_accuracy: 0.6554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:36:57,385]\u001b[0m Trial 8 finished with value: 0.6320261359214783 and parameters: {'lr': 0.000540630960807251}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "12/12 [==============================] - 17s 592ms/step - loss: 0.6944 - accuracy: 0.5078 - val_loss: 0.6906 - val_accuracy: 0.6214\n",
            "Epoch 2/3\n",
            "12/12 [==============================] - 6s 504ms/step - loss: 0.6908 - accuracy: 0.5359 - val_loss: 0.6901 - val_accuracy: 0.6397\n",
            "Epoch 3/3\n",
            "12/12 [==============================] - 6s 517ms/step - loss: 0.6912 - accuracy: 0.5255 - val_loss: 0.6894 - val_accuracy: 0.6397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-12-08 17:37:30,880]\u001b[0m Trial 9 finished with value: 0.630065381526947 and parameters: {'lr': 1.455130485302585e-05}. Best is trial 0 with value: 0.7790849804878235.\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 10\n",
            "Best trial:\n",
            "  Value: 0.7790849804878235\n",
            "  Params: \n",
            "    lr: 0.0025748915312877744\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10, timeout=600)\n",
        "\n",
        "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train_Glove_s, train_labels, test_size = 0.2)\n",
        "data_history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                           validation_data=(np.array(X_test),np.array(y_test)),\n",
        "                           epochs=20,\n",
        "                           batch_size=128,\n",
        "                           verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_graphs(axs, graph_index, history, string):\n",
        "    axs[graph_index].plot(history.history[string])\n",
        "    axs[graph_index].plot(history.history['val_'+string], '')\n",
        "    # axs[graph_index].xlabel(\"Epochs\")\n",
        "    # axs[graph_index].ylabel(string)\n",
        "    axs[graph_index].set(xlabel=\"Epochs\", ylabel=string)\n",
        "    axs[graph_index].legend([string, 'val_'+string])\n",
        "    # axs[graph_index].show()\n",
        "\n",
        "fig, axs = plt.subplots(2, figsize=(15, 15))\n",
        "                        \n",
        "plot_graphs(axs, 0, data_history, 'accuracy')\n",
        "plot_graphs(axs, 1, data_history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Script.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
