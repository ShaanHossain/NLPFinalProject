{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaanHossain/NLPFinalProject/blob/master/Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcjc9r0-zQI5",
        "outputId": "0e593be2-43ef-45d3-9fde-2a7bf63778c2"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "# from google.colab import drive # import drive from google colab\n",
        "# ROOT = \"/content/drive\"     # default location for the drive\n",
        "# print(ROOT)                 # print content of ROOT (Optional\n",
        "# drive.mount(ROOT)           # we mount the google drive at /content/drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Importing Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import files\n",
        "import sys\n",
        "from csv import reader\n",
        "from typing import List\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
        "import numpy as np\n",
        "import inflect\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Initial Setup - Defining the driving variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determines which dataset use and how much to use :\n",
        "# HateSpeech: Column-0 : Sentence, Column-1 : Label [noHate-0, Hate-1]\n",
        "# either 'HateSpeech' or 'KaggleTwitter' or 'TDavidson'\n",
        "dataset_to_use = \"HateSpeech\"\n",
        "dataset_percentage = 100  # percentage range 1 to 100\n",
        "\n",
        "# Initializes file path, column of csv file to parse and\n",
        "# the delimiter for parsing\n",
        "training_file = \"\"\n",
        "test_file = \"\"\n",
        "sentence_column_to_parse = None\n",
        "label_column_to_parse = None\n",
        "lancaster = LancasterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "delimiter = \",\"\n",
        "if dataset_to_use == \"HateSpeech\":\n",
        "    training_file = \"datasets/hate-speech/train.txt\"\n",
        "    test_file = \"datasets/hate-speech/test.txt\"\n",
        "    delimiter = \"\\t\"\n",
        "    sentence_column_to_parse = 0\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"KaggleTwitter\":\n",
        "    training_file = \"datasets/kaggle-twitter/train.csv\"\n",
        "    test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 2\n",
        "    label_column_to_parse = 1\n",
        "elif dataset_to_use == \"TDavidson\":\n",
        "    training_file = \"datasets/t-davidson-hate-speech/labeled_data.csv\"\n",
        "    # TODO: Update test path for this dataset\n",
        "    # test_file = \"datasets/kaggle-twitter/test.csv\"\n",
        "    sentence_column_to_parse = 6\n",
        "    label_column_to_parse = 2\n",
        "else:\n",
        "    print(\"Invalid Dataset specified\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Preprocessing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Seven tasks are done as part of this:\n",
        "  - lower word case\n",
        "  - remove stopwords\n",
        "  - remove punctuation\n",
        "  - convert numbers to texts\n",
        "  - perform stemming\n",
        "  - Add - \\<s> and \\</s> for every sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def replace_numbers(sentence:List[str]) -> List[str]:\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words\n",
        "    with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []  \n",
        "    for word in sentence.split():\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return \" \".join(new_words)\n",
        "\n",
        "def stem_words(sentence: List[str]) -> List[str]:\n",
        "    \"\"\"Stems the given sentence\n",
        "\n",
        "    Args:\n",
        "        sentence (list): words to be stemmed\n",
        "\n",
        "    Returns:\n",
        "        str: stemmed sentence\n",
        "    \"\"\"\n",
        "    stemmed_words = []\n",
        "    for word in sentence.split():\n",
        "        stemmed_words.append(lancaster.stem(word))\n",
        "    return \" \".join(stemmed_words)\n",
        "\n",
        "def preprocessing(running_lines: List[str]) -> List[str]:\n",
        "    \"\"\"This function takes in the running test and return back the\n",
        "    preprocessed text. Six tasks are done as part of this:\n",
        "      1. lower word case\n",
        "      2. remove stopwords\n",
        "      3. remove punctuation\n",
        "      4. convert numbers to texts\n",
        "      5. perform stemming\n",
        "\n",
        "    Args:\n",
        "        sentence (List[str]): list of lines\n",
        "\n",
        "    Returns:\n",
        "        List[str]: list of sentences which are processed\n",
        "    \"\"\"\n",
        "    preprocessed_lines = []\n",
        "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "    for line in running_lines:\n",
        "        # lower case\n",
        "        lower_case_data = line.lower()\n",
        "        # remove stop words\n",
        "        data_without_stop_word = remove_stopwords(lower_case_data)\n",
        "        # remove punctunation\n",
        "        data_without_punct = strip_punctuation(data_without_stop_word)\n",
        "        # replace numbers '1' to 'one'\n",
        "        processed_data = replace_numbers(data_without_punct)\n",
        "        # stem words\n",
        "        processed_data = stem_words(processed_data)\n",
        "        # add start and stop tags\n",
        "        # processed_data.insert(0, \"<s>\")\n",
        "        # processed_data.append(\"</s>\")\n",
        "        preprocessed_lines.append(processed_data)\n",
        "    return preprocessed_lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Parsing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_data(training_file_path: str, percentage: int,\n",
        "               sentence_column: int, label_column: int,\n",
        "               delimit: str):\n",
        "    \"\"\"This function is used to parse input lines\n",
        "    and returns a the provided percent of data.\n",
        "\n",
        "    Args:\n",
        "        lines (List[str]): list of lines\n",
        "        percentage (int): percent of the dataset needed\n",
        "        sentence_column (int): sentence column from the dataset\n",
        "        label_column (int): label column from the dataset\n",
        "        delimit (str): delimiter\n",
        "    Returns:\n",
        "        List[str], List[str]: examples , labels -> [percentage of dataset]\n",
        "    \"\"\"\n",
        "    percentage_sentences = []\n",
        "    percentage_labels = []\n",
        "    with open(training_file_path, \"r\", encoding=\"utf8\",\n",
        "              errors=\"ignore\") as csvfile:\n",
        "        read_sentences = []\n",
        "        label_sentences = []\n",
        "        csv_reader = reader(csvfile, delimiter=delimit)\n",
        "        # skipping header\n",
        "        header = next(csv_reader)\n",
        "        # line_length = len(list(csv_reader_copy))\n",
        "        if header is not None:\n",
        "            for row in csv_reader:\n",
        "                read_sentences.append(row[sentence_column])\n",
        "                label_sentences.append(int(row[label_column]))\n",
        "        end_of_data = int(len(read_sentences) * percentage * .01)\n",
        "        percentage_sentences = read_sentences[0:end_of_data]\n",
        "        percentage_labels = label_sentences[0:end_of_data]\n",
        "    return percentage_sentences, percentage_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing Improved - BiLSTM on HateSpeech with 100 % data \n"
          ]
        }
      ],
      "source": [
        "train_sentences, train_labels = parse_data(training_file,\n",
        "                                           dataset_percentage,\n",
        "                                           sentence_column_to_parse,\n",
        "                                           label_column_to_parse,\n",
        "                                           delimiter)\n",
        "# parse and preprocess the data\n",
        "processed_train_sentences = preprocessing(train_sentences)\n",
        "# verify the processed sentences\n",
        "# for sentence in sentences:\n",
        "#     print(sentence)\n",
        "# This is the baseline classifier\n",
        "print(\n",
        "    f\"Performing Improved - BiLSTM on {dataset_to_use}\"\n",
        "    f\" with {dataset_percentage} % data \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Building Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Generating word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this step, we intend to use the Keras library to build a recurrent neural network based on bidirectional LSTMs. The model will take word embeddings as input so we will use pre-trained GloVe embeddings to make the embedding dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sentence_word_embeddings(X_train_sentences:List[str]):\n",
        "    \"\"\"Converts the sentences into word embeddings.\n",
        "\n",
        "    Args:\n",
        "        X_train_sentences (List[str]): list of training sentences\n",
        "\n",
        "    Returns:\n",
        "        tuple: word embeddings for each sentence, vocab size and embedding dictionary\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    text = np.array(X_train_sentences)\n",
        "    tokenizer.fit_on_texts(X_train_sentences)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    embeddings_dict = {}\n",
        "    file_embeddings = open(\"utils/glove.twitter.27B.50d.txt\", encoding=\"utf8\")\n",
        "    for embedding_line in file_embeddings:\n",
        "        values = embedding_line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_dict[word] = coefs\n",
        "    file_embeddings.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (text, word_index, embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3951 unique tokens.\n",
            "Total 1193514 word vectors.\n"
          ]
        }
      ],
      "source": [
        "X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(processed_train_sentences)\n",
        "embedding_size = len(X_train_Glove_s[0])\n",
        "## Check function\n",
        "# x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "# X_train_Glove_s, word_index_s, embeddings_dict_s = convert_sentence_word_embeddings(x_train_sample)\n",
        "# print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "# print(\"\\n Word index of the word testing is : \", word_index_s[\"industry\"])\n",
        "# print(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Defining function to create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bilstm(word_index, embeddings_dict, nclasses,  MAX_SEQUENCE_LENGTH, EMBEDDING_DIM=50, dropout=0.5, hidden_layer = 3, lstm_node = 32):\n",
        "    model = Sequential()\n",
        "    # Make the embedding matrix using the embedding_dict\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    # Add embedding layer\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    # Add hidden layers \n",
        "    for i in range(0,hidden_layer):\n",
        "        # Add a bidirectional lstm layer\n",
        "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "        # Add a dropout layer after each lstm layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.2)))\n",
        "    model.add(Dropout(dropout))\n",
        "    # Add the fully connected layer with 256 nurons and relu activation\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    # Add the output layer with softmax activation since we have 2 classes\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    # Compile the model using sparse_categorical_crossentropy\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building Model!\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 143, 50)           197600    \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 143, 64)          21248     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 143, 64)           0         \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 143, 64)          24832     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 143, 64)           0         \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 143, 64)          24832     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 143, 64)           0         \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 64)               24832     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               16640     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 310,498\n",
            "Trainable params: 310,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "print(\"Building Model!\")\n",
        "model = build_bilstm(word_index_s, embeddings_dict_s, 2, embedding_size)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 2/12 [====>.........................] - ETA: 3:48 - loss: 0.6840 - accuracy: 0.5312 "
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_train_Glove_s, train_labels, test_size = 0.2)\n",
        "data_history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                           validation_data=(np.array(X_test),np.array(y_test)),\n",
        "                           epochs=5,\n",
        "                           batch_size=128,\n",
        "                           verbose=1)\n",
        "plot_graphs(data_history, 'accuracy')\n",
        "plot_graphs(data_history, 'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Script.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
